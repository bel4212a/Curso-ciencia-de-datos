{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paralelizacion de entrenamiento de redes neuronales con TensorFlow\n",
    "\n",
    "En esta seccion dejaremos atras los rudimentos de las matematicas y nos centraremos en utilizar TensorFlow, la cual es una de las librerias mas populares de arpendizaje profundo y que realiza una implementacion mas eficaz de las redes neuronales que cualquier otra implementacion de Numpy.\n",
    "\n",
    "TensorFlow es una interfaz de programacion multiplataforma y escalable para implementar y ejecutar algortimos de aprendizaje automatico de una manera mas eficaz ya que permite usar tanto la CPU como la GPU, la cual suele tener muchos mas procesadores que la CPU, los cuales, combinando sus frecuencias, presentan un rendimiento mas potente. La API mas desarrollada de esta herramienta se presenta para Python, por lo cual muchos desarrolladores se ven atraidos a este lenguaje.\n",
    "\n",
    "## Primeros pasos con TensorFlow\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/02.01-understanding-data-types.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T23:37:42.144677Z",
     "start_time": "2021-03-15T23:37:35.224942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creando tensores\n",
    "# =============================================\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "a = np.array([1, 2, 3], dtype=np.int32)\n",
    "b = [4, 5, 6]\n",
    "\n",
    "t_a = tf.convert_to_tensor(a)\n",
    "t_b = tf.convert_to_tensor(b)\n",
    "\n",
    "print(t_a)\n",
    "print(t_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T23:39:40.542083Z",
     "start_time": "2021-03-15T23:39:40.478254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Obteniendo las dimensiones de un tensor\n",
    "# ===============================================\n",
    "t_ones = tf.ones((2, 3))\n",
    "print(t_ones)\n",
    "t_ones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T23:41:01.011035Z",
     "start_time": "2021-03-15T23:41:01.002060Z"
    }
   },
   "outputs": [],
   "source": [
    "# Obteniendo los valores del tensor como array\n",
    "# ===============================================\n",
    "t_ones.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T23:42:12.474932Z",
     "start_time": "2021-03-15T23:42:12.458975Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creando un tensor de valores constantes\n",
    "# ================================================\n",
    "const_tensor = tf.constant([1.2, 5, np.pi], dtype=tf.float32)\n",
    "print(const_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T23:42:42.220775Z",
     "start_time": "2021-03-15T23:42:42.201830Z"
    }
   },
   "outputs": [],
   "source": [
    "matriz = np.array([[2, 3, 4, 5], [6, 7, 8, 8]], dtype = np.int32)\n",
    "matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T23:42:55.297264Z",
     "start_time": "2021-03-15T23:42:55.280309Z"
    }
   },
   "outputs": [],
   "source": [
    "matriz_tf = tf.convert_to_tensor(matriz)\n",
    "print(matriz_tf, end = '\\n'*2)\n",
    "print(matriz_tf.numpy(), end = '\\n'*2)\n",
    "print(matriz_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulando los tipos de datos y forma de un tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T23:44:25.200244Z",
     "start_time": "2021-03-15T23:44:25.192255Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cambiando el tipo de datos del tensor\n",
    "# ==============================================\n",
    "print(matriz_tf.dtype)\n",
    "\n",
    "matriz_tf_n = tf.cast(matriz_tf, tf.int64)\n",
    "\n",
    "print(matriz_tf_n.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T23:45:57.113934Z",
     "start_time": "2021-03-15T23:45:57.074047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transponiendo un tensor\n",
    "# =================================================\n",
    "t = tf.random.uniform(shape=(3, 5))\n",
    "print(t, end = '\\n'*2)\n",
    "\n",
    "t_tr = tf.transpose(t)\n",
    "print(t_tr, end = '\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T23:47:01.970944Z",
     "start_time": "2021-03-15T23:47:01.939066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Redimensionando un vector\n",
    "# =====================================\n",
    "t = tf.zeros((30,))\n",
    "print(t, end = '\\n'*2)\n",
    "print(t.shape, end = '\\n'*3)\n",
    "\n",
    "t_reshape = tf.reshape(t, shape=(5, 6))\n",
    "print(t_reshape, end = '\\n'*2)\n",
    "print(t_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T23:48:33.023817Z",
     "start_time": "2021-03-15T23:48:32.991908Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removiendo las dimensiones innecesarias\n",
    "# =====================================================\n",
    "t = tf.zeros((1, 2, 1, 4, 1))\n",
    "print(t, end = '\\n'*2)\n",
    "print(t.shape, end = '\\n'*3)\n",
    "\n",
    "t_sqz = tf.squeeze(t, axis=(2, 4))\n",
    "print(t_sqz, end = '\\n'*2)\n",
    "print(t_sqz.shape, end = '\\n'*3)\n",
    "print(t.shape, ' --> ', t_sqz.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones matematicas sobre tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T23:56:14.123880Z",
     "start_time": "2021-03-15T23:56:14.082995Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inicializando dos tensores con numeros aleatorios\n",
    "# =============================================================\n",
    "tf.random.set_seed(1)\n",
    "t1 = tf.random.uniform(shape=(5, 2), minval=-1.0, maxval=1.0)\n",
    "t2 = tf.random.normal(shape=(5, 2), mean=0.0, stddev=1.0)\n",
    "\n",
    "print(t1, '\\n'*2, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T23:58:31.346518Z",
     "start_time": "2021-03-15T23:58:31.324569Z"
    }
   },
   "outputs": [],
   "source": [
    "# Producto tipo element-wise: elemento a elemento\n",
    "# =================================================\n",
    "t3 = tf.multiply(t1, t2).numpy()\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:01:04.631720Z",
     "start_time": "2021-03-16T00:01:04.591835Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Promedio segun el eje\n",
    "# ================================================\n",
    "t4 = tf.math.reduce_mean(t1, axis=None)\n",
    "print(t4, end = '\\n'*3)\n",
    "\n",
    "t4 = tf.math.reduce_mean(t1, axis=0)\n",
    "print(t4, end = '\\n'*3)\n",
    "\n",
    "t4 = tf.math.reduce_mean(t1, axis=1)\n",
    "print(t4, end = '\\n'*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:02:54.929455Z",
     "start_time": "2021-03-16T00:02:54.892553Z"
    }
   },
   "outputs": [],
   "source": [
    "# suma segun el eje\n",
    "# =================================================\n",
    "t4 = tf.math.reduce_sum(t1, axis=None)\n",
    "print('Suma de todos los elementos:', t4, end = '\\n'*3)\n",
    "\n",
    "t4 = tf.math.reduce_sum(t1, axis=0)\n",
    "print('Suma de los elementos por columnas:', t4, end = '\\n'*3)\n",
    "\n",
    "t4 = tf.math.reduce_sum(t1, axis=1)\n",
    "print('Suma de los elementos por filas:', t4, end = '\\n'*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:03:43.618121Z",
     "start_time": "2021-03-16T00:03:43.573241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Desviacion estandar segun el eje\n",
    "# =================================================\n",
    "t4 = tf.math.reduce_std(t1, axis=None)\n",
    "print('Suma de todos los elementos:', t4, end = '\\n'*3)\n",
    "\n",
    "t4 = tf.math.reduce_std(t1, axis=0)\n",
    "print('Suma de los elementos por columnas:', t4, end = '\\n'*3)\n",
    "\n",
    "t4 = tf.math.reduce_std(t1, axis=1)\n",
    "print('Suma de los elementos por filas:', t4, end = '\\n'*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:06:44.436432Z",
     "start_time": "2021-03-16T00:06:44.379787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Producto entre matrices\n",
    "# ===========================================\n",
    "t5 = tf.linalg.matmul(t1, t2, transpose_b=True)\n",
    "print(t5.numpy(), end = '\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:07:08.663761Z",
     "start_time": "2021-03-16T00:07:08.653791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Producto entre matrices\n",
    "# ===========================================\n",
    "t6 = tf.linalg.matmul(t1, t2, transpose_a=True)\n",
    "print(t6.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:09:20.850405Z",
     "start_time": "2021-03-16T00:09:20.824477Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculando la norma de un vector\n",
    "# ==========================================\n",
    "norm_t1 = tf.norm(t1, ord=2, axis=None).numpy()\n",
    "print(norm_t1, end='\\n'*2)\n",
    "\n",
    "norm_t1 = tf.norm(t1, ord=2, axis=0).numpy()\n",
    "print(norm_t1, end='\\n'*2)\n",
    "\n",
    "norm_t1 = tf.norm(t1, ord=2, axis=1).numpy()\n",
    "print(norm_t1, end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partir, apilar y concatenar tensores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:10:48.083745Z",
     "start_time": "2021-03-16T00:10:48.061288Z"
    }
   },
   "outputs": [],
   "source": [
    "# Datos a trabajar\n",
    "# =======================================\n",
    "tf.random.set_seed(1)\n",
    "t = tf.random.uniform((6,))\n",
    "print(t.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:11:44.953807Z",
     "start_time": "2021-03-16T00:11:44.926886Z"
    }
   },
   "outputs": [],
   "source": [
    "# Partiendo el tensor en un numero determinado de piezas\n",
    "# ======================================================\n",
    "t_splits = tf.split(t, num_or_size_splits = 3)\n",
    "[item.numpy() for item in t_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:12:52.803113Z",
     "start_time": "2021-03-16T00:12:52.772195Z"
    }
   },
   "outputs": [],
   "source": [
    "# Partiendo el tensor segun los tamaños definidos\n",
    "# ======================================================\n",
    "tf.random.set_seed(1)\n",
    "t = tf.random.uniform((6,))\n",
    "print(t.numpy())\n",
    "t_splits = tf.split(t, num_or_size_splits=[3, 3])\n",
    "[item.numpy() for item in t_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:13:05.146994Z",
     "start_time": "2021-03-16T00:13:05.113094Z"
    }
   },
   "outputs": [],
   "source": [
    "print(matriz_tf.numpy())\n",
    "# m_splits = tf.split(t, num_or_size_splits = 0, axis = 1)\n",
    "matriz_n = tf.reshape(matriz_tf, shape = (8,))\n",
    "print(matriz_n.numpy())\n",
    "m_splits = tf.split(matriz_n, num_or_size_splits = 2)\n",
    "[item.numpy() for item in m_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:14:28.679268Z",
     "start_time": "2021-03-16T00:14:28.649348Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenando tensores\n",
    "# =========================================\n",
    "A = tf.ones((3,))\n",
    "print(A, end ='\\n'*2)\n",
    "\n",
    "B = tf.zeros((2,))\n",
    "print(B, end ='\\n'*2)\n",
    "\n",
    "C = tf.concat([A, B], axis=0)\n",
    "print(C.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:15:47.452207Z",
     "start_time": "2021-03-16T00:15:47.420298Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apilando tensores\n",
    "# =========================================\n",
    "A = tf.ones((3,))\n",
    "print(A, end ='\\n'*2)\n",
    "B = tf.zeros((3,))\n",
    "print(B, end ='\\n'*2)\n",
    "S = tf.stack([A, B], axis=1)\n",
    "print(S.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas funciones y herramientas en:\n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"burk\">\n",
    "EJERCICIOS</div><i class=\"fa fa-lightbulb-o \"></i>\n",
    "\n",
    "1. Cree dos tensores de dimensiones (4, 6), de numeros aleatorios provenientes de una distribucion normal estandar con promedio 0.0 y dsv 1.0. Imprimalos.\n",
    "2. Multiplique los anteriores tensores de las dos formas vistas, element-wise y producto matricial, realizando las dos transposiciones vistas. \n",
    "3. Calcule los promedios, desviaciones estandar y suma de sus elementos para los dos tensores.\n",
    "4. Redimensione los tensores para que sean ahora de rango 1.\n",
    "5. Calcule el coseno de los elementos de  los tensores (revise la documentacion).\n",
    "6. Cree un tensor de rango 1 con 1001 elementos, empezando con el 0 y hasta el 30.\n",
    "7. Realice un for sobre los elementos del tensor e imprimalos.\n",
    "8. Realice el calculo de los factoriales de los numero del 1 al 30 usando el tensor del punto 6. Imprima el resultado como un DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T23:08:27.899080Z",
     "start_time": "2021-03-14T23:08:27.880131Z"
    }
   },
   "source": [
    "# Creación de *pipelines* de entrada con tf.data: la API de conjunto de datos de TensorFlow\n",
    "\n",
    "Cuando entrenamos un modelo NN profundo, generalmente entrenamos el modelo de forma incremental utilizando un algoritmo de optimización iterativo como el descenso de gradiente estocástico, como hemos visto en clases anteriores.\n",
    "\n",
    "La API de Keras es un contenedor de TensorFlow para crear modelos NN. La API de Keras proporciona un método, `.fit ()`, para entrenar los modelos. En los casos en que el conjunto de datos de entrenamiento es bastante pequeño y se puede cargar como un tensor en la memoria, los modelos de TensorFlow (que se compilan con la API de Keras) pueden usar este tensor directamente a través de su método .fit () para el entrenamiento. Sin embargo, en casos de uso típicos, cuando el conjunto de datos es demasiado grande para caber en la memoria de la computadora, necesitaremos cargar los datos del dispositivo de almacenamiento principal (por ejemplo, el disco duro o la unidad de estado sólido) en trozos, es decir, lote por lote. \n",
    "\n",
    "Además, es posible que necesitemos construir un *pipeline* de procesamiento de datos para aplicar ciertas transformaciones y pasos de preprocesamiento a nuestros datos, como el centrado medio, el escalado o la adición de ruido para aumentar el procedimiento de entrenamiento y evitar el sobreajuste.\n",
    "\n",
    "Aplicar las funciones de preprocesamiento manualmente cada vez puede resultar bastante engorroso. Afortunadamente, TensorFlow proporciona una clase especial para construir *pipelines* de preprocesamiento eficientes y convenientes. En esta parte, veremos una descripción general de los diferentes métodos para construir un conjunto de datos de TensorFlow, incluidas las transformaciones del conjunto de datos y los pasos de preprocesamiento comunes.\n",
    "\n",
    "## Creando un Dataset de TensorFlow desde tensores existentes\n",
    "\n",
    "Si los datos ya existen en forma de un objeto tensor, una lista de Python o una matriz NumPy, podemos crear fácilmente un conjunto de datos usando la función `tf.data.Dataset.from_tensor_slices()`. Esta función devuelve un objeto de la clase Dataset, que podemos usar para iterar a través de los elementos individuales en el conjunto de datos de entrada:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T23:25:30.539885Z",
     "start_time": "2021-03-16T23:25:21.710079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Ejemplo con listas\n",
    "# ======================================================\n",
    "a = [1.2, 3.4, 7.5, 4.1, 5.0, 1.0]\n",
    "ds = tf.data.Dataset.from_tensor_slices(a)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T23:28:00.791323Z",
     "start_time": "2021-03-16T23:28:00.772316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.2, shape=(), dtype=float32)\n",
      "tf.Tensor(3.4, shape=(), dtype=float32)\n",
      "tf.Tensor(7.5, shape=(), dtype=float32)\n",
      "tf.Tensor(4.1, shape=(), dtype=float32)\n",
      "tf.Tensor(5.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "1.2\n",
      "3.4\n",
      "7.5\n",
      "4.1\n",
      "5.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for item in ds:\n",
    "    print(item)\n",
    "    \n",
    "for i in ds:\n",
    "    print(i.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos crear lotes a partir de este conjunto de datos, con un tamaño de lote deseado de 3, podemos hacerlo de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T23:30:43.198156Z",
     "start_time": "2021-03-16T23:30:43.174418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1: tf.Tensor([1.2 3.4 7.5], shape=(3,), dtype=float32)\n",
      "batch 2: tf.Tensor([4.1 5.  1. ], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Creando lotes de 3 elementos cada uno\n",
    "# ===================================================\n",
    "ds_batch = ds.batch(3)\n",
    "for i, elem in enumerate(ds_batch, 1):\n",
    "    print(f'batch {i}:', elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto creará dos lotes a partir de este conjunto de datos, donde los primeros tres elementos van al lote n° 1 y los elementos restantes al lote n° 2. El método `.batch()` tiene un argumento opcional, `drop_remainder`, que es útil para los casos en los que el número de elementos en el tensor no es divisible por el tamaño de lote deseado. El valor predeterminado de `drop_remainder` es `False`.\n",
    "\n",
    "## Combinar dos tensores en un Dataset\n",
    "\n",
    "A menudo, podemos tener los datos en dos (o posiblemente más) tensores. Por ejemplo, podríamos tener un tensor para características y un tensor para etiquetas. En tales casos, necesitamos construir un conjunto de datos que combine estos tensores juntos, lo que nos permitirá recuperar los elementos de estos tensores en tuplas.\n",
    "\n",
    "Suponga que tenemos dos tensores, t_x y t_y. El tensor t_x contiene nuestros valores de características, cada uno de tamaño 3, y t_y almacena las etiquetas de clase. Para este ejemplo, primero creamos estos dos tensores de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T23:32:04.012039Z",
     "start_time": "2021-03-16T23:32:03.986966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.16513085 0.9014813  0.6309742 ]\n",
      " [0.4345461  0.29193902 0.64250207]\n",
      " [0.9757855  0.43509948 0.6601019 ]\n",
      " [0.60489583 0.6366315  0.6144488 ]], shape=(4, 3), dtype=float32)\n",
      "tf.Tensor([0 1 2 3], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Datos de ejemplo\n",
    "# ============================================\n",
    "tf.random.set_seed(1)\n",
    "t_x = tf.random.uniform([4, 3], dtype=tf.float32)\n",
    "t_y = tf.range(4)\n",
    "print(t_x)\n",
    "print(t_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T23:33:23.478517Z",
     "start_time": "2021-03-16T23:33:23.437464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [0.16513085 0.9014813  0.6309742 ]  y: 0\n",
      "x: [0.4345461  0.29193902 0.64250207]  y: 1\n",
      "x: [0.9757855  0.43509948 0.6601019 ]  y: 2\n",
      "x: [0.60489583 0.6366315  0.6144488 ]  y: 3\n"
     ]
    }
   ],
   "source": [
    "# Uniendo los dos tensores en un Dataset\n",
    "# ============================================\n",
    "ds_x = tf.data.Dataset.from_tensor_slices(t_x)\n",
    "ds_y = tf.data.Dataset.from_tensor_slices(t_y)\n",
    "\n",
    "ds_joint = tf.data.Dataset.zip((ds_x, ds_y))\n",
    "\n",
    "for example in ds_joint:\n",
    "    print('x:', example[0].numpy(),' y:', example[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T23:34:24.551211Z",
     "start_time": "2021-03-16T23:34:24.505128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [0.16513085 0.9014813  0.6309742 ]  y: 0\n",
      "x: [0.4345461  0.29193902 0.64250207]  y: 1\n",
      "x: [0.9757855  0.43509948 0.6601019 ]  y: 2\n",
      "x: [0.60489583 0.6366315  0.6144488 ]  y: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((3,), ()), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_joint = tf.data.Dataset.from_tensor_slices((t_x, t_y))\n",
    "for example in ds_joint:\n",
    "    #print(example)\n",
    "    print('x:', example[0].numpy(), ' y:', example[1].numpy())\n",
    "\n",
    "ds_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T23:36:24.488807Z",
     "start_time": "2021-03-16T23:36:24.227413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x: [-0.6697383   0.80296254  0.26194835]  y: 0\n",
      " x: [-0.13090777 -0.41612196  0.28500414]  y: 1\n",
      " x: [ 0.951571   -0.12980103  0.32020378]  y: 2\n",
      " x: [0.20979166 0.27326298 0.22889757]  y: 3\n"
     ]
    }
   ],
   "source": [
    "# Operacion sobre el dataset generado\n",
    "# ====================================================\n",
    "ds_trans = ds_joint.map(lambda x, y: (x*2-1.0, y))\n",
    "for example in ds_trans: \n",
    "    print(' x:', example[0].numpy(), ' y:', example[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mezclar, agrupar y repetir\n",
    "\n",
    "Para entrenar un modelo NN usando la optimización de descenso de gradiente estocástico, es importante alimentar los datos de entrenamiento como lotes mezclados aleatoriamente. Ya hemos visto arriba como crear lotes llamando al método `.batch()` de un objeto de conjunto de datos. Ahora, además de crear lotes, vamos a mezclar y reiterar sobre los conjuntos de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:26:45.009397Z",
     "start_time": "2021-03-16T00:26:44.949611Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mezclando los elementos de un tensor\n",
    "# ===================================================\n",
    "tf.random.set_seed(1)\n",
    "ds = ds_joint.shuffle(buffer_size = len(t_x))\n",
    "for example in ds:\n",
    "    print(' x:', example[0].numpy(), ' y:', example[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T22:21:23.036313Z",
     "start_time": "2021-03-15T22:21:23.025308Z"
    }
   },
   "source": [
    "donde las filas se barajan sin perder la correspondencia uno a uno entre las entradas en x e y. El método `.shuffle()` requiere un argumento llamado `buffer_size`, que determina cuántos elementos del conjunto de datos se agrupan antes de barajar. Los elementos del búfer se recuperan aleatoriamente y su lugar en el búfer se asigna a los siguientes elementos del conjunto de datos original (sin mezclar). Por lo tanto, si elegimos un tamaño de búfer pequeño, es posible que no mezclemos perfectamente el conjunto de datos.\n",
    "\n",
    "Si el conjunto de datos es pequeño, la elección de un tamaño de búfer relativamente pequeño puede afectar negativamente el rendimiento predictivo del NN, ya que es posible que el conjunto de datos no esté completamente aleatorizado. En la práctica, sin embargo, por lo general no tiene un efecto notable cuando se trabaja con conjuntos de datos relativamente grandes, lo cual es común en el aprendizaje profundo.\n",
    "\n",
    "Alternativamente, para asegurar una aleatorización completa durante cada época, simplemente podemos elegir un tamaño de búfer que sea igual al número de ejemplos de entrenamiento, como en el código anterior (`buffer_size = len(t_x)`).\n",
    "\n",
    " Ahora, creemos lotes a partir del conjunto de datos ds_joint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:26:47.795112Z",
     "start_time": "2021-03-16T00:26:47.776161Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = ds_joint.batch(batch_size = 3, drop_remainder = False)\n",
    "print(ds)\n",
    "batch_x, batch_y = next(iter(ds))\n",
    "print('Batch-x:\\n', batch_x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:26:50.218322Z",
     "start_time": "2021-03-16T00:26:50.213336Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Batch-y: ', batch_y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, al entrenar un modelo para múltiples épocas, necesitamos mezclar e iterar sobre el conjunto de datos por el número deseado de épocas. Entonces, repitamos el conjunto de datos por lotes dos veces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:26:51.953195Z",
     "start_time": "2021-03-16T00:26:51.924314Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = ds_joint.batch(3).repeat(count = 2)\n",
    "for i,(batch_x, batch_y) in enumerate(ds):\n",
    "    print(i, batch_x.numpy(), batch_y.numpy(), end = '\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto da como resultado dos copias de cada lote. Si cambiamos el orden de estas dos operaciones, es decir, primero lote y luego repetimos, los resultados serán diferentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:26:53.682413Z",
     "start_time": "2021-03-16T00:26:53.656484Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "ds = ds_joint.repeat(count=2).batch(3)\n",
    "for i,(batch_x, batch_y) in enumerate(ds):\n",
    "    print(i, batch_x.numpy(), batch_y.numpy(), end = '\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, para comprender mejor cómo se comportan estas tres operaciones (batch, shuffle y repeat), experimentemos con ellas en diferentes órdenes. Primero, combinaremos las operaciones en el siguiente orden: (1) shuffle, (2) batch y (3) repeat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:27:15.728820Z",
     "start_time": "2021-03-16T00:27:15.696905Z"
    }
   },
   "outputs": [],
   "source": [
    "# Orden 1: shuffle -> batch -> repeat\n",
    "tf.random.set_seed(1)\n",
    "ds = ds_joint.shuffle(4).batch(2).repeat(3)\n",
    "for i,(batch_x, batch_y) in enumerate(ds):\n",
    "    print(i, batch_x, batch_y.numpy(), end = '\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:28:07.074667Z",
     "start_time": "2021-03-16T00:28:07.037771Z"
    }
   },
   "outputs": [],
   "source": [
    "# Orden 2: batch -> shuffle -> repeat\n",
    "tf.random.set_seed(1)\n",
    "ds = ds_joint.batch(2).shuffle(4).repeat(3)\n",
    "for i,(batch_x, batch_y) in enumerate(ds):\n",
    "    print(i, batch_x, batch_y.numpy(), end = '\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:29:12.495520Z",
     "start_time": "2021-03-16T00:29:12.468561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Orden 2: batch -> repeat-> shuffle\n",
    "tf.random.set_seed(1)\n",
    "ds = ds_joint.batch(2).repeat(3).shuffle(4)\n",
    "for i,(batch_x, batch_y) in enumerate(ds):\n",
    "    print(i, batch_x, batch_y.numpy(), end = '\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obteniendo conjuntos de datos disponibles de la biblioteca tensorflow_datasets\n",
    "\n",
    "La biblioteca tensorflow_datasets proporciona una buena colección de conjuntos de datos disponibles gratuitamente para entrenar o evaluar modelos de aprendizaje profundo. Los conjuntos de datos están bien formateados y vienen con descripciones informativas, incluido el formato de características y etiquetas y su tipo y dimensionalidad, así como la cita del documento original que introdujo el conjunto de datos en formato BibTeX. Otra ventaja es que todos estos conjuntos de datos están preparados y listos para usar como objetos tf.data.Dataset, por lo que todas las funciones que cubrimos se pueden usar directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T01:14:38.389028Z",
     "start_time": "2021-03-16T01:14:38.381057Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T01:15:00.029816Z",
     "start_time": "2021-03-16T01:14:58.075170Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(len(tfds.list_builders()))\n",
    "print(tfds.list_builders()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T01:37:44.489390Z",
     "start_time": "2021-03-16T01:37:00.769967Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Trabajando con el archivo mnist\n",
    "# ===============================================\n",
    "mnist, mnist_info = tfds.load('mnist', with_info=True, shuffle_files=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T01:38:39.615198Z",
     "start_time": "2021-03-16T01:38:39.607275Z"
    }
   },
   "outputs": [],
   "source": [
    "print(mnist_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T01:39:08.811745Z",
     "start_time": "2021-03-16T01:39:08.806758Z"
    }
   },
   "outputs": [],
   "source": [
    "print(mnist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T01:45:32.485050Z",
     "start_time": "2021-03-16T01:45:32.420194Z"
    }
   },
   "outputs": [],
   "source": [
    "ds_train = mnist['train']\n",
    "ds_train = ds_train.map(lambda item:(item['image'], item['label']))\n",
    "ds_train = ds_train.batch(10)\n",
    "batch = next(iter(ds_train))\n",
    "print(batch[0].shape, batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T01:43:19.450046Z",
     "start_time": "2021-03-16T01:43:19.062807Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "for i,(image,label) in enumerate(zip(batch[0], batch[1])):\n",
    "    ax = fig.add_subplot(2, 5, i+1)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(image[:, :, 0], cmap='gray_r')\n",
    "    ax.set_title('{}'.format(label), size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T01:32:59.052286Z",
     "start_time": "2021-03-16T01:32:59.030346Z"
    }
   },
   "source": [
    "# Construyendo un modelo NN en TensorFlow\n",
    "\n",
    "## La API de TensorFlow Keras (tf.keras)\n",
    "\n",
    "Keras es una API NN de alto nivel y se desarrolló originalmente para ejecutarse sobre otras bibliotecas como TensorFlow y Theano. Keras proporciona una interfaz de programación modular y fácil de usar que permite la creación de prototipos y la construcción de modelos complejos en solo unas pocas líneas de código. Keras se puede instalar independientemente de PyPI y luego configurarse para usar TensorFlow como su motor de backend. Keras está estrechamente integrado en TensorFlow y se puede acceder a sus módulos a través de tf.keras.\n",
    "\n",
    "En TensorFlow 2.0, tf.keras se ha convertido en el enfoque principal y recomendado para implementar modelos. Esto tiene la ventaja de que admite funcionalidades específicas de TensorFlow, como las canalizaciones de conjuntos de datos que usan tf.data.\n",
    "\n",
    "La API de Keras (tf.keras) hace que la construcción de un modelo NN sea extremadamente fácil. El enfoque más utilizado para crear una NN en TensorFlow es a través de `tf.keras.Sequential()`, que permite apilar capas para formar una red. Se puede dar una pila de capas en una lista de Python a un modelo definido como tf.keras.Sequential(). Alternativamente, las capas se pueden agregar una por una usando el método .add().\n",
    "\n",
    "Además, tf.keras nos permite definir un modelo subclasificando tf.keras.Model.\n",
    "\n",
    "Esto nos da más control sobre la propagacion hacia adelante al definir el método call() para nuestra clase modelo para especificar la propagacion hacia adelante explicitamente. \n",
    "\n",
    "Finalmente, los modelos construidos usando la API tf.keras se pueden compilar y entrenar a través de los métodos .compile() y .fit()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo  un modelo de regresion lineal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T21:22:33.313085Z",
     "start_time": "2021-03-16T21:22:33.308345Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.arange(10).reshape((10, 1))\n",
    "y_train = np.array([1.0, 1.3, 3.1, 2.0, 5.0, 6.3, 6.6, 7.4, 8.0, 9.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T21:22:52.304374Z",
     "start_time": "2021-03-16T21:22:52.299521Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T21:23:35.006693Z",
     "start_time": "2021-03-16T21:23:34.325166Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X_train, y_train, 'o', markersize=10)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T21:26:30.488706Z",
     "start_time": "2021-03-16T21:26:30.472738Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X_train_norm = (X_train - np.mean(X_train))/np.std(X_train)\n",
    "ds_train_orig = tf.data.Dataset.from_tensor_slices((tf.cast(X_train_norm, tf.float32),tf.cast(y_train, tf.float32)))\n",
    "\n",
    "for i in ds_train_orig:\n",
    "    print(i[0].numpy(), i[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, podemos definir nuestro modelo de regresión lineal como $𝑧 = 𝑤x + 𝑏$. Aquí, vamos a utilizar la API de Keras. `tf.keras` proporciona capas predefinidas para construir modelos NN complejos, pero para empezar, usaremos un modelo desde cero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T21:31:02.993466Z",
     "start_time": "2021-03-16T21:31:02.988829Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.w = tf.Variable(0.0, name='weight')\n",
    "        self.b = tf.Variable(0.0, name='bias')\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.w * x + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T21:31:34.590016Z",
     "start_time": "2021-03-16T21:31:34.549185Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "model.build(input_shape=(None, 1))\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
