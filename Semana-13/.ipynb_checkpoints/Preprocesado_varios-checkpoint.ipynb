{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratar con datos ausentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las bases de datos de fenomenos reales suelen presentar valores faltantes por diversas razones, como por ejemplo, datos dejados como vacios en encuestas, valores no medibles, errores al ingresar la informacion, entreo otros. Es bastante normal encontrar este tipo de valores, y sin embargo pueden representar un serio problema a la hora de ingresarlos a los algoritmos de prediccion o clasificacion. En python los valores faltantes se conocen y observan como `NaN`, y en bases de datos relacionales, se conocen como `NULL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T19:25:01.742011Z",
     "start_time": "2021-02-11T19:25:00.137017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creando un conjunto de datos de ejemplo\n",
    "# ====================================================\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "csv_data = \\\n",
    "'''A,B,C,D\n",
    "1.0,2.0,3.0,4.0\n",
    "5.0,6.0,,8.0\n",
    "10.0,11.0,12.0,'''\n",
    "\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconocer que hay valores NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T19:25:48.824935Z",
     "start_time": "2021-02-11T19:25:48.810000Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T19:27:32.871456Z",
     "start_time": "2021-02-11T19:27:32.863769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Con el parametro axis se modifica sobre quien se hace la suma. Por defecto: axis = 0\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T19:34:19.247492Z",
     "start_time": "2021-02-11T19:34:18.019475Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('train.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T19:38:49.857419Z",
     "start_time": "2021-02-11T19:38:49.805592Z"
    }
   },
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('titanic.csv')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/titanic/data?select=train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T19:39:13.489222Z",
     "start_time": "2021-02-11T19:39:13.476294Z"
    }
   },
   "outputs": [],
   "source": [
    "df3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T19:40:11.228990Z",
     "start_time": "2021-02-11T19:40:11.216019Z"
    }
   },
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T19:40:41.374330Z",
     "start_time": "2021-02-11T19:40:41.296504Z"
    }
   },
   "outputs": [],
   "source": [
    "df3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratar con los valores NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar las muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T19:41:50.117681Z",
     "start_time": "2021-02-11T19:41:50.103753Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T19:44:31.766660Z",
     "start_time": "2021-02-11T19:44:31.752731Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T19:43:34.775494Z",
     "start_time": "2021-02-11T19:43:34.761534Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T19:44:36.908772Z",
     "start_time": "2021-02-11T19:44:36.892850Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(how = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T19:45:46.078371Z",
     "start_time": "2021-02-11T19:45:46.066402Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(thresh = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T19:46:17.357191Z",
     "start_time": "2021-02-11T19:46:17.341235Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(subset = ['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputar las muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:00:02.073444Z",
     "start_time": "2021-02-11T20:00:02.061514Z"
    }
   },
   "outputs": [],
   "source": [
    "# imputacion por medias\n",
    "# ========================================\n",
    "dfN = df.copy()\n",
    "from sklearn.impute import SimpleImputer\n",
    "imr = SimpleImputer(missing_values = np.nan, strategy='mean')\n",
    "imr.fit(dfN.values)\n",
    "dfN = imr.transform(dfN)\n",
    "dfN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T19:59:27.030173Z",
     "start_time": "2021-02-11T19:59:27.023192Z"
    }
   },
   "source": [
    "Lo anterior solo funciona para caracteristicas numericas; para caracteristicas categoricas existe la opcion `strategy='most_frequent'`, la cyal tomara la caracteristica mas frecuente para reemplazar los valores NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:05:04.535749Z",
     "start_time": "2021-02-11T20:05:04.508756Z"
    }
   },
   "outputs": [],
   "source": [
    "imr = SimpleImputer(strategy = 'most_frequent')\n",
    "df3N = imr.fit_transform(df3.values)\n",
    "df3N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajar con datos categoricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchas bases de datos contienen tipos de datos categoricos referentes a cualidades, tallas, generos, etc. Este tipo de datos puede interferir en los algoritmos de aprendizaje automatico, y aunque varios de ellos tienen funciones implicitas para tratar con ellos, se considera buena practica el realizar su tratamiento previo a enviarlos a los algotirmos de aprendizaje.\n",
    "\n",
    "Para esto, hemos de reconocer que existen dos tipos de datos categoricos:\n",
    "\n",
    "**Ordinales**: Son aquellos que se pueden ordenar de mayor a menor, por ejemplo las tallas de la ropa.\n",
    "\n",
    "**Nominales**: Son aquellos que no se pueden ordenar, ejemplo, los colores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:12:05.409360Z",
     "start_time": "2021-02-11T20:12:05.391342Z"
    }
   },
   "outputs": [],
   "source": [
    "# Datos de ejemplo\n",
    "# =====================================\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([['green', 'M', 10.1, 'class1'],\n",
    "                   ['red', 'L', 13.5, 'class2'],\n",
    "                   ['blue', 'XL', 15.3, 'class1']])\n",
    "\n",
    "df.columns = ['color', 'size', 'price', 'classlabel']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapear caracteristicas ordinales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el anterior ejemplo, debemos asegurarnos de que los datos de la columna `size` sean transformados a enteros, pero de tal forma que se reconozca el orden implicito en ellos. No existe una libreria o implementacion que reconozca dicho orden, pero es un tema relativamente facil de resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:17:29.439196Z",
     "start_time": "2021-02-11T20:17:29.423265Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mapeando los valores categoricos a enteros\n",
    "# ==================================================\n",
    "size_mapping = {'XL': 3, 'L': 2, 'M': 1}\n",
    "\n",
    "df['size'] = df['size'].map(size_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:19:25.805233Z",
     "start_time": "2021-02-11T20:19:25.795259Z"
    }
   },
   "outputs": [],
   "source": [
    "# Realizando un mapeo inverso\n",
    "# =======================================================\n",
    "inv_size_mapping = {v: k for k, v in size_mapping.items()}\n",
    "#inv_size_mapping\n",
    "#size_mapping.items()\n",
    "df['size'].map(inv_size_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapear etiquetas de clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:41:29.559221Z",
     "start_time": "2021-02-11T20:41:29.550380Z"
    }
   },
   "outputs": [],
   "source": [
    "# Metodo 1: Asignar las etiquetas de clase \"manualmente\"\n",
    "# =======================================================================\n",
    "import numpy as np\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(df['classlabel']))}\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:43:03.463252Z",
     "start_time": "2021-02-11T20:43:03.446300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mapeando los valores categoricos a enteros\n",
    "# ==================================================\n",
    "df['classlabel'] = df['classlabel'].map(class_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:43:36.895022Z",
     "start_time": "2021-02-11T20:43:36.883509Z"
    }
   },
   "outputs": [],
   "source": [
    "# Realizando un mapeo inverso\n",
    "# =======================================================\n",
    "inv_class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "df['classlabel'] = df['classlabel'].map(inv_class_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:45:12.104912Z",
     "start_time": "2021-02-11T20:45:12.096022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Metodo 2: Usando una libreria dedicada a la codificacion\n",
    "# ======================================================\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "class_le = LabelEncoder()\n",
    "y = class_le.fit_transform(df['classlabel'].values)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:46:14.968056Z",
     "start_time": "2021-02-11T20:46:14.960123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Realizando la transformacion inversa\n",
    "# ======================================================\n",
    "class_le.inverse_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapear caracteristicas nominales: \"Codificacion en caliente\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la seccion anterior, transformamos las etiquetas de clase nominales a enteros utilizando `LabelEncoder`; el resultado fue la asignacion de numeros enteros de 0 en adelante a las etiquetas de clase. Esto es aceptable ya que las etiquetas de clase no participan en la seleccion de caracteristicas para el modelo, ni en la actualizacion de los pesos, por lo tanto no es importante si una resulta ser calificada como mayor a las otras, aunque en realidad no lo sea.\n",
    "\n",
    "Otro es el caso cuando se busca la codificacion de caracteristicas nominales, pues estas si participan en el proceso de seleccion de las etiquetas, y por lo tanto el que sean ordenables puede ser erronamente indicativo para el modelo, realizando una mala interpretacion de la relacion de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T21:08:42.852806Z",
     "start_time": "2021-02-11T21:08:42.844183Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df[['color', 'size', 'price']].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T20:55:54.899180Z",
     "start_time": "2021-02-11T20:55:54.893163Z"
    }
   },
   "outputs": [],
   "source": [
    "color_le = LabelEncoder()\n",
    "X[:, 0] = color_le.fit_transform(X[:, 0])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T21:17:50.238639Z",
     "start_time": "2021-02-11T21:17:50.227801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Codificacion en caliente\n",
    "# Se crearan objetos del tipo (blue, green, red), de tal forma que blue sera: (1, 0, 0)\n",
    "# ===================================\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "ohe = make_column_transformer((OneHotEncoder(), [0]), remainder = 'passthrough')\n",
    "Xn = ohe.fit_transform(X)\n",
    "Xn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T21:08:11.321434Z",
     "start_time": "2021-02-11T21:08:11.314965Z"
    }
   },
   "source": [
    "Es importante tener en cuenta que las caracteristicas asi codificadas son redundantes, lo que implica la prescencia de colinealidad entre las caracteristicas. Lo adecuado, ademas de lo ya hecho, es incluir el parametro `drop = 'first` dentro de la declaracion del codificador : `OneHotEncoder(drop = 'first)`, y asi se eliminara la primera columna, previniendo entonces este posible error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T21:20:27.927854Z",
     "start_time": "2021-02-11T21:20:27.908905Z"
    }
   },
   "outputs": [],
   "source": [
    "# Codificacion en caliente usando pandas :)\n",
    "# ======================================================\n",
    "pd.get_dummies(df[['price', 'color', 'size']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T21:20:53.291934Z",
     "start_time": "2021-02-11T21:20:53.273018Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(df[['price', 'color', 'size']], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir el conjunto de datos en entrenamiento y test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustar las caracteristicas a las misma escala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que el escalado se realiza para que las caracteristicas sean comparables, y por lo tanto que todas sean significativas para el modelo. Los algoritmos de arboles de decision y bosques aleatorios son invariantes frente al escalado, pero la gran mayoria de algorritmos si se ven beneficiados de esta practica.\n",
    "\n",
    "Existen dos enfoques principales para escalar carateriticas:\n",
    "\n",
    "**Escalado min-max**: Normaliza los datos, es decir, los lleva a valores entre 0 y 1.\n",
    "\n",
    "$$x_{norm}^{(i)} = \\frac{x^{(i)}-x_{min}}{x_{max}-x_{min}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T21:28:05.422444Z",
     "start_time": "2021-02-11T21:28:05.414498Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "XN= mms.fit_transform(X[:, 1:])\n",
    "XN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T21:26:59.320004Z",
     "start_time": "2021-02-11T21:26:59.313439Z"
    }
   },
   "source": [
    "**Estandarizacion**: Transforma los datos a una distribucion normal estandar. Mas adecuado para los algoritmos de aprendizaje automatico.\n",
    "\n",
    "$$x_{std}^{(i)} = \\frac{x^{(i)}-\\mu_x}{\\sigma_x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T21:34:09.173873Z",
     "start_time": "2021-02-11T21:34:09.164934Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "XN = sc.fit_transform(X[:, 1:])\n",
    "XN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T21:41:18.461584Z",
     "start_time": "2021-02-11T21:41:18.445697Z"
    }
   },
   "outputs": [],
   "source": [
    "ex = np.array([0, 1, 2, 3, 4, 5])\n",
    "\n",
    "standardized = (ex - ex.mean()) / ex.std()\n",
    "normalized = (ex - ex.min()) / (ex.max() - ex.min())\n",
    "index = np.arange(len(standardized))\n",
    "\n",
    "pd.DataFrame({'standardized': standardized, 'normalized':normalized})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccionar las caracteristicas significativas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que si el rendimiento en el conjunto de entrenamiento es mayor que el rendimiento en el conjunto de prueba, esto es un fuerte indicativo de sobreajuste; para tratar con esta situacion existen varias posibilidades:\n",
    "\n",
    "* Recoger mas datos de entrenamiento.\n",
    "* Introducir una penalizacion para la complejidad mediante la regularizacion.\n",
    "* Elegir un modelo mas sencillo con menos parametros.\n",
    "* Reducir la dimensionalidad de los datos. (Seleccionar caracteristicas significativas)\n",
    "\n",
    "De todas ellas, la primera es usualmente la menos aplicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizaciones $L_1$ y $L_2$ como penalizacion contra la complejidad del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En clases pasadas habiamos definido la regularizacion $L_2$ como penalizacion para reducir la complejidad de los modelos. La definicion matematica usada entonces fue:\n",
    "\n",
    "$$L_2 = ||\\textbf{w}||^2_2 = \\sum_{j=1}^{m} w_j^2$$\n",
    "\n",
    "La regularizacion $L_1$ se define como:\n",
    "\n",
    "$$L_1 = ||\\textbf{w}||_1 = \\sum_{j=1}^{m} |w_j|$$\n",
    "\n",
    "La regularizacion $L_1$ suele producir soluciones dispersas, es decir, con mayor prescencia de ceros que del resto de elementos. Esto puede ser util se tenemos muchas caracteristicas irrelevantes, y en ese sentido, la regularizacion $L_1$ puede ser util para la seleccion de caracteristicas significativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T21:57:06.351311Z",
     "start_time": "2021-02-11T21:57:05.742010Z"
    }
   },
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)\n",
    "\n",
    "df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n",
    "                   'Proline']\n",
    "\n",
    "print('Class labels', np.unique(df_wine['Class label']))\n",
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T21:58:15.445496Z",
     "start_time": "2021-02-11T21:58:15.433531Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T21:58:34.610573Z",
     "start_time": "2021-02-11T21:58:34.604586Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "X_test_std = stdsc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T22:06:33.701125Z",
     "start_time": "2021-02-11T22:06:33.693147Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l1', solver = 'liblinear', C=1.0)\n",
    "lr.fit(X_train_std, y_train)\n",
    "print('Training accuracy:', lr.score(X_train_std, y_train))\n",
    "print('Test accuracy:', lr.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T22:06:35.434456Z",
     "start_time": "2021-02-11T22:06:35.429468Z"
    }
   },
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T22:06:37.307935Z",
     "start_time": "2021-02-11T22:06:37.295000Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(lr.coef_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
